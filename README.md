# Olympic-Azure-ETL-pipeline

## Project Overview  
This project implements an **ETL pipeline** to process Tokyo Olympic data using **Microsoft Azure's cloud ecosystem**. The pipeline follows these key stages:  

1. **Data Ingestion:** Data is sourced and ingested using **Azure Data Factory**, which automates the data movement and orchestration.  
2. **Storage:** The ingested raw data is stored in **Azure Data Lake Gen2** for scalable and secure data storage.  
3. **Transformation:** **Azure Databricks** is used for data cleaning, transformation, and enrichment.  
4. **Analytics & Querying:** The transformed data is stored back in **Azure Data Lake Gen2**, and **Azure Synapse Analytics** is used to run **SQL queries** for data analysis and insights.  
5. **Visualization & Dashboarding:** The processed data is fed into visualization tools such as **Power BI, Looker Studio, and Tableau** to create interactive dashboards for insights.  

## Key Technologies  
- **Azure Data Factory** (Data Ingestion)  
- **Azure Data Lake Gen2** (Storage)  
- **Azure Databricks** (Data Transformation)  
- **Azure Synapse Analytics** (SQL-based Analytics)  
- **Power BI, Looker Studio, Tableau** (Visualization & Reporting)  

This pipeline ensures an efficient, scalable, and automated approach to handling Olympic data, enabling advanced analytics and visualization.


![alt_text](https://github.com/sahilbishnoi26/Olympic-Azure-ETL-Pipeline/blob/main/img1.png)
