# Olympic-Azure-ETL-pipeline

## Description:
This project implements an ETL pipeline to process Tokyo Olympic data using Microsoft Azure's cloud ecosystem. The pipeline follows these key stages:

Data Ingestion: Data is sourced and ingested using Azure Data Factory, which automates the data movement and orchestration.
Storage: The ingested raw data is stored in Azure Data Lake Gen2 for scalable and secure data storage.
Transformation: Azure Databricks is used for data cleaning, transformation, and enrichment.
Analytics & Querying: The transformed data is stored back in Azure Data Lake Gen2, and Azure Synapse Analytics is used to run SQL queries for data analysis and insights.
Visualization & Dashboarding: The processed data is fed into visualization tools such as Power BI, Looker Studio, and Tableau to create interactive dashboards for insights.

![alt_text](https://github.com/sahilbishnoi26/Olympic-Azure-ETL-Pipeline/blob/main/img1.png)
